# Clustering and classification

## Data description

The dataset presents the Housing Values in Suburbs of Boston. It includes 506 rows and 14 columns. IT can be downloaded from MASS package.


The following variables are included:

- crim - per capita crime rate by town
- zn - proportion of residential land zoned for lots over 25,000 sq.ft
- indus - proportion of non-retail business acres per town
- chas - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
- nox - nitrogen oxides concentration (parts per 10 million)
- rm - average number of rooms per dwelling
- age - proportion of owner-occupied units built prior to 1940
- dis - weighted mean of distances to five Boston employment centres
- rad - index of accessibility to radial highways
- tax - full-value property-tax rate per \$10,000
- ptratio - pupil-teacher ratio by town
- black - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- lstat - lower status of the population (percent)
- medv - median value of owner-occupied homes in \$1000s


```{r}
# access the MASS package
library(MASS)
# load the data
data("Boston")
str(Boston)
dim(Boston) # 506*14
```


## Graphical overview and summaries

Let's explore the data and check the distribution

```{r}
library(ggplot2)
library(GGally)
library(tidyr)
gather(Boston) %>% ggplot(aes(x = value))  + geom_histogram(aes(y = ..density..), colour="black", fill="white", bins = 18,) + facet_wrap("key", scales = "free")
summary(Boston)
```


From the plots we can conclude that:

- the variables black, chas,crim,ptratio, zn are highly skewed to different sides: black (skewed to right) shows high propotion of black people, chas (skewed to the left - 0) shows the track doesn't bound the river, crim (skewed to the left) shows that the crime rate is low, ptratio(skewed to the right) shows that pupil-teacher ratio is rather high, zn (skewed to the left) shows that proportion of residential land zoned for lots over 25,000 sq.ft is low.


Let's look at the correlations between variables 
```{r}
library(MASS)
library(tidyr)
library(corrplot)
# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)
cor_matrix
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```


From the plots we can the following:

1) The strong positive correlation is between:

- full-value property-tax rate per\$10,000 **tax** and index of accessibility to radial highways **rad**: **0.91**
- nitrogen oxides concentration **nox** and proportion of non-retail business acres per town **indus**: **0.76**
- nitrogen oxides concentration **nox** and proportion of owner-occupied units built prior to 1940 **age**: **0.73**

2) The strong negative correlation is between:

-  weighted mean of distances to five Boston employment centres **dis** and nitrogen oxides concentration **nox**: **-0.77**
- weighted mean of distances to five Boston employment centres **dis** and  proportion of owner-occupied units built prior to 1940 **age**: **-0.75**
- median value of owner-occupied homes **medv** and lower status of the population **lstar**: **-0.74**


## Data wrangling

Let's scale the data

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)
summary(boston_scaled)
class(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
```


We can see the following changes after scaling:
all the means equal zero.

Let's create a categorical variable of the crime rate
```{r}
# create a quantile vector of crime and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
c <- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, label = c, include.lowest = TRUE)
table(crime)
# remove original crime variable from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```


Let's divide the data into the train and test sets

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set 
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```


## Linear discriminant analysis (LDA)

For linear discriminant analysis we use the categorical crime rate as the target variable and all the other variables as predictor variables.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)
```

Let's draw the LDA plot
```{r}
plot(lda.fit, dimen = 2, col = classes, pch = classes )
lda.arrows(lda.fit, myscale = 2)
```

## Prediction for LDA

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

In general this model shows good results for low and high crime level prediction but doesn't work so good with the middle rate.

## K-means

To perform k-means algorithm on our dataset let's scale the data again
```{r}
boston_scaled2 <- scale(Boston)
boston_scaled2 <- as.data.frame(boston_scaled2)
summary(boston_scaled2)
```

Now let's calculate the distances between the observations with Euclidean distance method
```{r}
# euclidean distance matrix
dist_eu2 <- dist(boston_scaled2) # function uses Euclidean by default
summary(dist_eu2)
str(dist_eu2)
```

Now we run k-means algorithm on our dataset. Through that we should investigate what is the optimal number of clusters. To determine the number of clusters we should have a look at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes.

The optimal number of clusters is when the value of total WCSS changes radically. Then using two clusters seems to be optimal.

```{r}
library(GGally)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled2, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <- kmeans(boston_scaled2, centers = 2)
# plot the result
ggpairs(boston_scaled2[0:7], aes(color = as.factor(km$cluster)))
ggpairs(boston_scaled2[8:14], aes(color = as.factor(km$cluster)))
```

We can see that 2 clusters is enough to show the difference between classes. The most evident differnece is in indus, nox, tax, rad, dis, ptratio.
